{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# AI2 - Assignment 4 - SQuAD - QA - BERT\n\n__Student's Credentials__: Vissarion Moutafis - sdi1800119\n\nIn this notebook we will fine tune *DistilBERT* model, on SQuAD dataset. We will use **exact match** and **soft f1 score** as evaluation metrics and we will provide the user with a routing that answers questions based on a context.\n","metadata":{"id":"T5aVFTm-6Ku-"}},{"cell_type":"markdown","source":"### Loading Hugging Face\nTo start with, we will import utilities and download the dataset from transformers library","metadata":{"id":"2YPF2_3Kv11z"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn\nimport re\nseaborn.set_style(\"ticks\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report\n\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nimport torch\nimport torch.nn as nn\nimport torchtext\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n!pip install transformers datasets\nimport transformers\nfrom transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\nfrom datasets import load_dataset, load_metric\n\n!pip install tqdm\nfrom tqdm import tqdm, trange\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"Dg1ERqGqvYYM","outputId":"09bb1a48-b3e5-4df4-e151-1b83afc4aaa7","execution":{"iopub.status.busy":"2022-02-21T15:19:01.370008Z","iopub.execute_input":"2022-02-21T15:19:01.370523Z","iopub.status.idle":"2022-02-21T15:19:27.857383Z","shell.execute_reply.started":"2022-02-21T15:19:01.370494Z","shell.execute_reply":"2022-02-21T15:19:27.856141Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\nCollecting datasets\n  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n     |████████████████████████████████| 311 kB 592 kB/s            \n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.1.0)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nCollecting xxhash\n  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n     |████████████████████████████████| 243 kB 8.8 MB/s            \n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nInstalling collected packages: xxhash, datasets\nSuccessfully installed datasets-1.18.3 xxhash-2.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.62.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\ncuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"datasets = load_dataset(\"squad_v2\")\nmodel_checkpoint = 'distilbert-base-uncased'","metadata":{"id":"RXZvNwt7bLS6","outputId":"6f657a22-4459-4983-9281-24c59a660e28","execution":{"iopub.status.busy":"2022-02-21T15:19:27.859934Z","iopub.execute_input":"2022-02-21T15:19:27.860232Z","iopub.status.idle":"2022-02-21T15:19:48.290733Z","shell.execute_reply.started":"2022-02-21T15:19:27.860170Z","shell.execute_reply":"2022-02-21T15:19:48.289708Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6135f5364a14bae9d13821e6c0d43cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a05eb3209f84d1bb9b2ffca26060e0b"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4f4e9afd82402ab94abc354cae4b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e107faa2d74518adc99c0aac4e1340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c21a52377cd47db9a476a2ee10e927c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5027b87eacf74773a8d42afdba4c0e85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2481a8016b9b40f6a36a198e83b922b4"}},"metadata":{}}]},{"cell_type":"markdown","source":"After that we will need a tokenizer. We will use the default *bistil-bert-uncased* tokenizer instance from transformers library.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nMAX_LENGTH = 384\nDOC_STRIDE = 128 # multi-context overlapping range for large context'd instances ","metadata":{"id":"-aUzvZ1ex-XL","execution":{"iopub.status.busy":"2022-02-21T15:19:48.292260Z","iopub.execute_input":"2022-02-21T15:19:48.293271Z","iopub.status.idle":"2022-02-21T15:19:55.903559Z","shell.execute_reply.started":"2022-02-21T15:19:48.293228Z","shell.execute_reply":"2022-02-21T15:19:55.902619Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5affb315d5e4685bc62d5da326e81fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d7441d390148d4ad483d67029b7020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5884f288578d4e7cb563173a467dca13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd1b75edf284e89acd0401cce935a98"}},"metadata":{}}]},{"cell_type":"markdown","source":"To preprocess the data we will:\n- concat the query and the context\n- tokenize the context and the query \n- use overflow mappings to enable multi context answers, since there might be answers that overflow the context tokens' length\n- use offset mapping to get start and end positions of answer in tokenized input","metadata":{}},{"cell_type":"code","source":"def preprocess_squad(examples):\n  # get the questions and the context\n  questions = [q.strip() for q in examples[\"question\"]]\n  context = examples[\"context\"]\n  # tokenize questions along with the context \n  inputs = tokenizer(\n        questions,\n        context,\n        max_length=MAX_LENGTH,\n        stride=DOC_STRIDE,\n        truncation=\"only_second\",\n        padding=\"max_length\",\n        return_offsets_mapping=True,\n        return_overflowing_tokens=True\n    )\n  offset_mapping = inputs.pop(\"offset_mapping\")\n  sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n  answers = examples[\"answers\"]\n  start_positions = []\n  end_positions = []\n  \n  for i, offset in enumerate(offset_mapping):\n    sample_index = sample_mapping[i]\n    answer = examples[\"answers\"][sample_index]\n    # if there is no answer default to [CLS]\n    if not answer[\"answer_start\"]:\n      start_positions.append(inputs['input_ids'][i].index(tokenizer.cls_token_id))\n      end_positions.append(inputs['input_ids'][i].index(tokenizer.cls_token_id))\n      continue \n    \n    # get answer start and end positions\n    start_char = answer[\"answer_start\"][0]\n    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n    sequence_ids = inputs.sequence_ids(i)\n\n    # Find the start and end of the context\n    idx = 0\n    while sequence_ids[idx] != 1:\n      idx += 1\n    context_start = idx\n    while sequence_ids[idx] == 1:\n      idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label it (0, 0)\n    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n      start_positions.append(inputs['input_ids'][i].index(tokenizer.cls_token_id))\n      end_positions.append(inputs['input_ids'][i].index(tokenizer.cls_token_id))\n    else:\n      # Otherwise it's the start and end token positions\n      idx = context_start\n      while idx <= context_end and offset[idx][0] <= start_char:\n        idx += 1\n      start_positions.append(idx - 1)\n\n      idx = context_end\n      while idx >= context_start and offset[idx][1] >= end_char:\n        idx -= 1\n      end_positions.append(idx + 1)\n\n  inputs[\"start_positions\"] = start_positions\n  inputs[\"end_positions\"] = end_positions\n  return inputs\n","metadata":{"id":"A_gHAKsbgHkH","execution":{"iopub.status.busy":"2022-02-21T15:19:55.906282Z","iopub.execute_input":"2022-02-21T15:19:55.906605Z","iopub.status.idle":"2022-02-21T15:19:55.921623Z","shell.execute_reply.started":"2022-02-21T15:19:55.906566Z","shell.execute_reply":"2022-02-21T15:19:55.920629Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Map the whole dataset and define train/test datasets and data loaders.","metadata":{}},{"cell_type":"code","source":"dataset = datasets.map(preprocess_squad, batched=True, remove_columns=datasets['train'].column_names)\n","metadata":{"id":"p0fowMyvx93a","outputId":"fb71d2bd-fdbd-466c-e0f4-725a8d0b44a7","execution":{"iopub.status.busy":"2022-02-21T15:19:55.923303Z","iopub.execute_input":"2022-02-21T15:19:55.923933Z","iopub.status.idle":"2022-02-21T15:22:06.061396Z","shell.execute_reply.started":"2022-02-21T15:19:55.923889Z","shell.execute_reply":"2022-02-21T15:22:06.060280Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18565572aa604b48a817d21f7edbd28f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f645587a432a4c36b07b5acf2eed6518"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = dataset['train']\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)","metadata":{"id":"5hVtuIQ4wBoc","execution":{"iopub.status.busy":"2022-02-21T15:22:06.063527Z","iopub.execute_input":"2022-02-21T15:22:06.064010Z","iopub.status.idle":"2022-02-21T15:22:06.071016Z","shell.execute_reply.started":"2022-02-21T15:22:06.063966Z","shell.execute_reply":"2022-02-21T15:22:06.069892Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_dataset = dataset['validation']\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{"id":"-_dl3wc256V0","execution":{"iopub.status.busy":"2022-02-21T15:22:06.073569Z","iopub.execute_input":"2022-02-21T15:22:06.073918Z","iopub.status.idle":"2022-02-21T15:22:06.163330Z","shell.execute_reply.started":"2022-02-21T15:22:06.073875Z","shell.execute_reply":"2022-02-21T15:22:06.162260Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Model Creation\n\nWe will use the default checkpoint model from transformers library and train it for 2 epochs in order to fine tune it on SQuAD.v2 dataset","metadata":{"id":"ycH9nbYXT6Bc"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"fu7JyjIx_O3p","execution":{"iopub.status.busy":"2022-02-21T15:22:06.165778Z","iopub.execute_input":"2022-02-21T15:22:06.166153Z","iopub.status.idle":"2022-02-21T15:22:06.174687Z","shell.execute_reply.started":"2022-02-21T15:22:06.166107Z","shell.execute_reply":"2022-02-21T15:22:06.173467Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"id":"pIrLlCyLUGF7","outputId":"d4c8d9d2-e67c-4789-970e-7ef6b83fbf0b","execution":{"iopub.status.busy":"2022-02-21T15:22:06.176315Z","iopub.execute_input":"2022-02-21T15:22:06.176699Z","iopub.status.idle":"2022-02-21T15:22:21.740415Z","shell.execute_reply.started":"2022-02-21T15:22:06.176646Z","shell.execute_reply":"2022-02-21T15:22:21.739266Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be21d25e59f4790b999c6811e01646c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"id":"eabIHsY75w4Z","execution":{"iopub.status.busy":"2022-02-21T15:22:21.744576Z","iopub.execute_input":"2022-02-21T15:22:21.744859Z","iopub.status.idle":"2022-02-21T15:22:25.211657Z","shell.execute_reply.started":"2022-02-21T15:22:21.744817Z","shell.execute_reply":"2022-02-21T15:22:25.210651Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def training_step(model, optimizer, epoch_i, train_loader, history=None):\n  torch.cuda.empty_cache()\n  # 1 step of backprop with train/test error estimation\n  total_loss = 0\n  acc = []\n  pbar = tqdm(train_loader)\n  for i,batch in enumerate(pbar):\n    torch.cuda.empty_cache()\n    # set the gradients to zero for new estimation\n    optimizer.zero_grad() \n    # forward pass\n    args = {\n        \"start_positions\" : torch.LongTensor(batch[\"start_positions\"]).to(device),\n        \"end_positions\" : torch.LongTensor(batch[\"end_positions\"]).to(device),\n        \"input_ids\" : torch.stack(batch[\"input_ids\"], axis=1).to(device),\n        \"attention_mask\" : torch.stack(batch[\"attention_mask\"], axis=1).to(device),\n    }\n    outputs = model(**args) \n    loss = outputs[0]\n    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n    # compute loss \n    total_loss += loss.item()\n\n    # backpropagate error\n    loss.backward()\n    # apply gradient clipping adjust model's parameters\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n    optimizer.step()\n    \n    acc.append(((start_pred == args['start_positions']).sum()/len(start_pred)).item())\n    acc.append(((end_pred == args['end_positions']).sum()/len(end_pred)).item())\n    pbar.set_description('Epoch {}: train loss: {}, train accuracy: {}%'.format(epoch_i, total_loss/(i+1), sum(acc)/len(acc)), refresh=True)\n    \n  train_loss = total_loss/len(train_loader)\n    \n    # if the user provides with a history dict the training step will save the current epoch's train-test loss\n  if history is not None: \n    history['train'].append(train_loss)\n  \n  test_loss = 0\n  acc = []\n  pbar = tqdm(test_loader)\n  for i,batch in enumerate(pbar):\n    torch.cuda.empty_cache()\n    with torch.no_grad():\n      args = {\n          \"start_positions\" : torch.LongTensor(batch[\"start_positions\"]).to(device),\n          \"end_positions\" : torch.LongTensor(batch[\"end_positions\"]).to(device),\n          \"input_ids\" : torch.stack(batch[\"input_ids\"], axis=1).to(device),\n          \"attention_mask\" : torch.stack(batch[\"attention_mask\"], axis=1).to(device)\n        }\n      outputs = model(**args)\n      test_loss += outputs[0].item()\n      start_pred = torch.argmax(outputs['start_logits'], dim=1)\n      end_pred = torch.argmax(outputs['end_logits'], dim=1)\n      acc.append(((start_pred == args['start_positions']).sum()/len(start_pred)).item())\n      acc.append(((end_pred == args['end_positions']).sum()/len(end_pred)).item())\n      pbar.set_description('Epoch {}: test loss {}%, test accuracy {}%'.format(epoch_i, test_loss/(i+1), sum(acc)/len(acc)), refresh=True)\n        \n  if history is not None: \n    history['test'].append(test_loss/len(test_loader))\n    \n\n  \n  return train_loss, test_loss/len(test_loader), sum(acc)/len(acc)\n\ndef train_model(history, model, train_loader, epochs, _lr):\n  # set to training mode\n  model.train()\n  criterion = torch.nn.CrossEntropyLoss()\n  optimizer = torch.optim.AdamW(model.parameters(), lr = _lr)\n\n  for epoch in range(epochs):\n    training_step(model, optimizer, epoch, train_loader, history)\n","metadata":{"id":"5xgnCWUAxTKw","execution":{"iopub.status.busy":"2022-02-21T15:22:25.213495Z","iopub.execute_input":"2022-02-21T15:22:25.213774Z","iopub.status.idle":"2022-02-21T15:22:25.249823Z","shell.execute_reply.started":"2022-02-21T15:22:25.213734Z","shell.execute_reply":"2022-02-21T15:22:25.247950Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"history = {'train':[], 'test':[]}\ntrain_model(history, model, train_loader, 2,  2e-5)","metadata":{"id":"9Xze0LMOzde4","outputId":"61de9a77-b5de-4469-f98f-9f20d8f5e58f","execution":{"iopub.status.busy":"2022-02-21T15:22:25.253239Z","iopub.execute_input":"2022-02-21T15:22:25.254199Z","iopub.status.idle":"2022-02-21T17:32:35.553082Z","shell.execute_reply.started":"2022-02-21T15:22:25.254139Z","shell.execute_reply":"2022-02-21T17:32:35.552014Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Epoch 0: train loss: 1.605350584211484, train accuracy: 0.5286812469645459%: 100%|██████████| 4118/4118 [1:03:20<00:00,  1.08it/s] \nEpoch 0: test loss 1.3018481034981577%, test accuracy 0.585416666692809%: 100%|██████████| 380/380 [01:41<00:00,  3.73it/s] \nEpoch 1: train loss: 1.0334340147149395, train accuracy: 0.6735103509057315%: 100%|██████████| 4118/4118 [1:03:25<00:00,  1.08it/s]\nEpoch 1: test loss 1.2703646647302729%, test accuracy 0.6111979167712362%: 100%|██████████| 380/380 [01:41<00:00,  3.73it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation\n\nWe will print the evaluation stats (metrics and training loss curves) for our model. After that we will provide the QA routine.","metadata":{"id":"CA52zGpeUGt2"}},{"cell_type":"code","source":"model.eval()","metadata":{"id":"pSv7RM33UKV0","outputId":"0bc5329b-2dba-48b5-f569-2bb465fe1297","execution":{"iopub.status.busy":"2022-02-21T17:32:35.555322Z","iopub.execute_input":"2022-02-21T17:32:35.555669Z","iopub.status.idle":"2022-02-21T17:32:35.570387Z","shell.execute_reply.started":"2022-02-21T17:32:35.555625Z","shell.execute_reply":"2022-02-21T17:32:35.569438Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\ntrain_loss = history['train']\ntest_loss = history['test']\nax.plot(np.arange(len(train_loss)), train_loss, test_loss)\nax.legend(['train_loss', 'validation_loss'])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:32:35.572037Z","iopub.execute_input":"2022-02-21T17:32:35.572466Z","iopub.status.idle":"2022-02-21T17:32:35.889147Z","shell.execute_reply.started":"2022-02-21T17:32:35.572397Z","shell.execute_reply":"2022-02-21T17:32:35.887990Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<matplotlib.legend.Legend at 0x7f71138c7e50>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# compute exact match based on identity opretion for torch.Tensor\ndef compute_exact_match(prediction, truth):\n    return ((prediction == truth).sum()/len(truth)).item()\n\n# compute soft f1 score based on calculating common tokens ocurrences \ndef compute_f1(pred_token_ranges, truth_token_ranges):\n    tmp_f1 = []\n    for i in range(pred_token_ranges.shape[0]):\n      start_p, end_p = int(pred_token_ranges[i][0]), int(pred_token_ranges[i][1])\n      start_t, end_t = int(true_token_ranges[i][0]), int(true_token_ranges[i][1])\n      common_tokens = max(min(end_t, end_p) - max(start_t, start_p) + 1, 0)\n      if common_tokens == 0:\n        tmp_f1.append(0)\n        continue\n      prec = common_tokens / (end_p - start_p + 1)\n      rec = common_tokens / (end_t - start_t + 1)\n      tmp_f1.append(2 * (prec * rec) / (prec + rec))\n    return sum(tmp_f1)/len(tmp_f1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:32:35.891040Z","iopub.execute_input":"2022-02-21T17:32:35.891970Z","iopub.status.idle":"2022-02-21T17:32:35.903136Z","shell.execute_reply.started":"2022-02-21T17:32:35.891919Z","shell.execute_reply":"2022-02-21T17:32:35.901951Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"f1_total = []\nacc = []\ntest_loss = []\nacc = []\npbar = tqdm(test_loader)\nfor batch in pbar:\n  torch.cuda.empty_cache()\n  with torch.no_grad():\n    args = {\n          \"start_positions\" : torch.LongTensor(batch[\"start_positions\"]).to(device),\n          \"end_positions\" : torch.LongTensor(batch[\"end_positions\"]).to(device),\n          \"input_ids\" : torch.stack(batch[\"input_ids\"], axis=1).to(device),\n          \"attention_mask\" : torch.stack(batch[\"attention_mask\"], axis=1).to(device)\n        }\n    outputs = model(**args)\n    test_loss.append(outputs[0].item())\n    # find the indices for start and end in every example of the batch\n    start_positions = []\n    end_positions = []\n    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n    # calculate exact match\n    acc.append(compute_exact_match(start_pred, args['start_positions']))\n    acc.append(compute_exact_match(end_pred, args['end_positions']))\n    \n    # create token ranges in the form of (start, end) and calculate f1 score\n    pred_token_ranges = torch.stack((start_pred, end_pred), axis=1)\n    true_token_ranges = torch.stack((args['start_positions'], args['end_positions']), axis=1)\n    \n    f1_total.append(compute_f1(pred_token_ranges, true_token_ranges))\n    pbar.set_description('Test loss {}, test accuracy {}, F1-Score={}'.format(sum(test_loss)/len(test_loss), round(sum(acc)/len(acc), 2), round(np.mean(f1_total),2)), refresh=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:32:35.905126Z","iopub.execute_input":"2022-02-21T17:32:35.906188Z","iopub.status.idle":"2022-02-21T17:34:15.887842Z","shell.execute_reply.started":"2022-02-21T17:32:35.906110Z","shell.execute_reply":"2022-02-21T17:34:15.886916Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Test loss 1.2570781939908078, test accuracy 0.62, F1-Score=0.62: 100%|██████████| 380/380 [01:39<00:00,  3.80it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# if you want to load your own model do it here (useful after first training session, 2h long)\n# model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:34:26.102289Z","iopub.execute_input":"2022-02-21T17:34:26.104972Z","iopub.status.idle":"2022-02-21T17:34:26.111923Z","shell.execute_reply.started":"2022-02-21T17:34:26.104927Z","shell.execute_reply":"2022-02-21T17:34:26.110518Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## The Question Answer Routine\n\nFollowing we give a routine that a user can answer queries based on a context. All you have to do is provide with a context and a question and we are ready to go.","metadata":{}},{"cell_type":"code","source":"def answer_q(question, context):\n    # tokenize question and context\n    inputs = tokenizer(\n      question,\n      context,\n      max_length=MAX_LENGTH,\n      stride=DOC_STRIDE,\n      truncation=\"only_second\",\n      padding=\"max_length\",\n      return_overflowing_tokens=True\n    )\n    \n    # predict answer\n    args = {\n          \"input_ids\" : torch.LongTensor(inputs[\"input_ids\"]).to(device),\n          \"attention_mask\" : torch.LongTensor(inputs[\"attention_mask\"]).to(device)\n      }\n    outputs = model(**args)\n    \n    # acquire the answer with the highest\n    position_scores = {}\n    context_idx = inputs.sequence_ids().index(1)\n    for start_pos, start_conf in enumerate(outputs['start_logits'][0][context_idx:].reshape(-1).detach().tolist()):\n        for end_pos, end_conf in enumerate(outputs['end_logits'][0][context_idx:].reshape(-1).detach().tolist()):\n            position_scores[start_pos+context_idx, end_pos+context_idx] = start_conf+end_conf\n            \n    start_pos, end_pos = max(position_scores.keys(), key=lambda p : position_scores[p])\n    \n    answer = tokenizer.decode(inputs['input_ids'][0][start_pos:end_pos+1])\n    if answer == '[CLS]' or answer == '':\n        answer = 'Impossible to answer.'\n    answer = answer.replace('[CLS]', '')\n    answer = answer.replace('[SEP]', '')\n\n    return answer","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:34:26.115674Z","iopub.execute_input":"2022-02-21T17:34:26.116353Z","iopub.status.idle":"2022-02-21T17:34:26.138021Z","shell.execute_reply.started":"2022-02-21T17:34:26.116309Z","shell.execute_reply":"2022-02-21T17:34:26.137073Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"context = 'BERT has its origins from pre-training contextual representations including semi-supervised sequence learning,[14] generative pre-training, ELMo,[15] and ULMFit.[16] Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, where BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.'\nquestions = ['What are BERT\\'origins?', \n             'What is BERT?', \n             'How context free models deal with words?', \n             'What\\'s your favorite desert?']\nfor q in questions:\n    print(f'Question: {q}\\nAnswer: {answer_q(q, context)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T17:34:26.143165Z","iopub.execute_input":"2022-02-21T17:34:26.146812Z","iopub.status.idle":"2022-02-21T17:34:26.876673Z","shell.execute_reply.started":"2022-02-21T17:34:26.146768Z","shell.execute_reply":"2022-02-21T17:34:26.874667Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Question: What are BERT'origins?\nAnswer: pre - training contextual representations\n\nQuestion: What is BERT?\nAnswer: a deeply bidirectional, unsupervised language representation\n\nQuestion: How context free models deal with words?\nAnswer: embedding representation for each word in the vocabulary, where bert takes into account the context for each occurrence of a given word\n\nQuestion: What's your favorite desert?\nAnswer: elmo, [ 15 ] and ulmfit\n\n","output_type":"stream"}]},{"cell_type":"code","source":"context = 'James Patrick Page OBE (born 9 January 1944)[1][2] \\n \\\nis an English musician, songwriter, multi-instrumentalist and \\n \\\nrecord producer who achieved international success as the guitarist and founder of the rock band Led Zeppelin. '\nq = 'For what band, did Page play for?'\nprint(f'Question: {q}\\nAnswer: {answer_q(q, context)}\\n')\nq = 'What is Page\\' full-name?'\nprint(f'Question: {q}\\nAnswer: {answer_q(q, context)}\\n')\nq = 'When was Page born?'\nprint(f'Question: {q}\\nAnswer: {answer_q(q, context)}\\n')","metadata":{"id":"cQOQ6qJv6sDi","execution":{"iopub.status.busy":"2022-02-21T17:34:26.878260Z","iopub.execute_input":"2022-02-21T17:34:26.879372Z","iopub.status.idle":"2022-02-21T17:34:27.276469Z","shell.execute_reply.started":"2022-02-21T17:34:26.879318Z","shell.execute_reply":"2022-02-21T17:34:27.275470Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Question: For what band, did Page play for?\nAnswer: led zeppelin\n\nQuestion: What is Page' full-name?\nAnswer: james patrick page obe\n\nQuestion: When was Page born?\nAnswer: 9 january 1944\n\n","output_type":"stream"}]}]}